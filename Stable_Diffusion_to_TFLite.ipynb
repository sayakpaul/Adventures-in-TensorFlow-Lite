{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayakpaul/Adventures-in-TensorFlow-Lite/blob/master/Stable_Diffusion_to_TFLite.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "FmQg42CPc8V7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIYdn1woOS1n",
        "outputId": "3d398c11-556a-40b5-cb5c-61794d8c4f97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.0/586.0 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.8/615.8 KB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.9/440.9 KB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.9.2 requires flatbuffers<2,>=1.12, but you have flatbuffers 23.1.21 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -U tf-nightly keras_cv -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "Bs-3OVK_c9_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras_cv\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5nTDz9hWT5n",
        "outputId": "38b22d46-0b07-490a-8b44-e25d38a605d9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You do not have Waymo Open Dataset installed, so KerasCV Waymo metrics are not available.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"KerasCV version: {keras_cv.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSXVkUKKaMTj",
        "outputId": "4c01a4ab-c4ea-47e9-ac4b-132dab658b09"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.12.0-dev20230121\n",
            "KerasCV version: 0.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Stable Diffusion"
      ],
      "metadata": {
        "id": "LI4e1iPTc_OB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras_cv.models.StableDiffusion(img_width=512, img_height=512)\n",
        "diffusion_model = model.diffusion_model\n",
        "text_encoder = model.text_encoder\n",
        "decoder = model.decoder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvOut1Q0XHkg",
        "outputId": "d3fa3322-b5fc-4a3d-ff9e-6f4d194b055d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "By using this model checkpoint, you acknowledge that its usage is subject to the terms of the CreativeML Open RAIL-M license at https://raw.githubusercontent.com/CompVis/stable-diffusion/main/LICENSE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conversions"
      ],
      "metadata": {
        "id": "V72cRVEydDyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "converter_text = tf.lite.TFLiteConverter.from_keras_model(text_encoder)\n",
        "converter_text.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_text_encoder = converter_text.convert()\n",
        "\n",
        "with open(\"text_encoder.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_text_encoder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqAPIX2IXSM6",
        "outputId": "9666f13a-3b3f-4d24-de1f-c8969af4017c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, layer_normalization_48_layer_call_fn while saving (showing 5 of 220). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "converter_decoder = tf.lite.TFLiteConverter.from_keras_model(decoder)\n",
        "converter_decoder.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_decoder = converter_decoder.convert()\n",
        "\n",
        "with open(\"image_decoder.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_decoder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1k14aTCXxZP",
        "outputId": "bb472c2a-7a08-40d7-c6ee-32d1a9df91c4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_84_layer_call_fn, conv2d_84_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, conv2d_85_layer_call_fn, conv2d_85_layer_call_and_return_conditional_losses while saving (showing 5 of 246). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "converter_diffusion = tf.lite.TFLiteConverter.from_keras_model(diffusion_model)\n",
        "converter_diffusion.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_diffusion = converter_diffusion.convert()\n",
        "\n",
        "with open(f\"diffusion_model.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_diffusion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cBR82GEYJRJ",
        "outputId": "e431b71a-70a3-49c9-ac28-2cc3725a9df8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, group_normalization_2_layer_call_fn, group_normalization_2_layer_call_and_return_conditional_losses while saving (showing 5 of 1320). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh *.tflite"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-OoD1cJaZhE",
        "outputId": "cebb48a4-0330-4db1-d87a-1f0b9ab84b3d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 824M Jan 23 11:31 diffusion_model.tflite\n",
            "-rw-r--r-- 1 root root  48M Jan 23 11:23 image_decoder.tflite\n",
            "-rw-r--r-- 1 root root 118M Jan 23 11:22 text_encoder.tflite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "iiKBxVUidFpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = model.tokenizer"
      ],
      "metadata": {
        "id": "HzwbCxzhatQC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modified from \n",
        "# https://github.com/freedomtan/keras_cv_stable_diffusion_to_tflite/blob/main/text_to_image_using_converted_tflite_models_dynamic.ipynb\n",
        "from keras_cv.models.stable_diffusion.constants import _UNCONDITIONAL_TOKENS\n",
        "from keras_cv.models.stable_diffusion.constants import _ALPHAS_CUMPROD\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "class StableDiffusionTFLite:\n",
        "    MAX_PROMPT_LENGTH = 77\n",
        "    IMG_HEIGHT = 512\n",
        "    IMG_WIDTH = 512\n",
        "    \n",
        "    def _get_initial_alphas(self, timesteps):\n",
        "        alphas = [_ALPHAS_CUMPROD[t] for t in timesteps]\n",
        "        alphas_prev = [1.0] + alphas[:-1]\n",
        "\n",
        "        return alphas, alphas_prev\n",
        "    \n",
        "    def _get_initial_diffusion_noise(self, batch_size, seed):\n",
        "        if seed is not None:\n",
        "            return tf.random.stateless_normal(\n",
        "                (batch_size, self.IMG_HEIGHT // 8, self.IMG_WIDTH // 8, 4),\n",
        "                seed=[seed, seed],\n",
        "            )\n",
        "        else:\n",
        "            return tf.random.normal(\n",
        "                (batch_size, self.IMG_HEIGHT // 8, self.IMG_WIDTH // 8, 4)\n",
        "            )\n",
        "        \n",
        "    def _get_timestep_embedding(self, timestep, batch_size, dim=320, max_period=10000):\n",
        "        half = dim // 2\n",
        "        freqs = tf.math.exp(\n",
        "            -math.log(max_period) * tf.range(0, half, dtype=tf.float32) / half\n",
        "        )\n",
        "        args = tf.convert_to_tensor([timestep], dtype=tf.float32) * freqs\n",
        "        embedding = tf.concat([tf.math.cos(args), tf.math.sin(args)], 0)\n",
        "        embedding = tf.reshape(embedding, [1, -1])\n",
        "        return tf.repeat(embedding, batch_size, axis=0)\n",
        "\n",
        "    def _get_pos_ids(self):\n",
        "        return tf.convert_to_tensor([list(range(self.MAX_PROMPT_LENGTH))], dtype=tf.int32)\n",
        "\n",
        "    def encoded_token_padded(self, prompt):\n",
        "        inputs = tokenizer.encode(prompt)\n",
        "        phrase = inputs + [49407] * (self.MAX_PROMPT_LENGTH - len(inputs))\n",
        "        phrase = tf.convert_to_tensor([phrase], dtype=tf.int32)\n",
        "\n",
        "        return phrase, self._get_pos_ids()\n",
        "    \n",
        "    def encode_text(self, prompt):\n",
        "        i_text_encoder = tf.lite.Interpreter('text_encoder.tflite')\n",
        "        i_text_encoder.allocate_tensors()\n",
        "        input_details = i_text_encoder.get_input_details()\n",
        "        output_details = i_text_encoder.get_output_details()\n",
        "\n",
        "        token, pos = self.encoded_token_padded(prompt)\n",
        "        i_text_encoder.set_tensor(input_details[0]['index'], token)\n",
        "        i_text_encoder.set_tensor(input_details[1]['index'], pos)\n",
        "\n",
        "        i_text_encoder.invoke()\n",
        "\n",
        "        output_data = i_text_encoder.get_tensor(output_details[0]['index'])\n",
        "\n",
        "        return output_data\n",
        "    \n",
        "    def encode_text_2(self, token, pos):\n",
        "        i_text_encoder = tf.lite.Interpreter('text_encoder.tflite')\n",
        "        i_text_encoder.allocate_tensors()\n",
        "        input_details = i_text_encoder.get_input_details()\n",
        "        output_details = i_text_encoder.get_output_details()\n",
        "\n",
        "        i_text_encoder.set_tensor(input_details[0]['index'], token)\n",
        "        i_text_encoder.set_tensor(input_details[1]['index'], pos)\n",
        "\n",
        "        i_text_encoder.invoke()\n",
        "\n",
        "        output_data = i_text_encoder.get_tensor(output_details[0]['index'])\n",
        "\n",
        "        return output_data\n",
        "\n",
        "\n",
        "    def _expand_tensor(self, text_embedding, batch_size):\n",
        "        \"\"\"Extends a tensor by repeating it to fit the shape of the given batch size.\"\"\"\n",
        "        text_embedding = tf.squeeze(text_embedding)\n",
        "        if text_embedding.shape.rank == 2:\n",
        "            text_embedding = tf.repeat(\n",
        "                tf.expand_dims(text_embedding, axis=0), batch_size, axis=0\n",
        "            )\n",
        "        return text_embedding\n",
        "    \n",
        "    def _get_unconditional_context(self):\n",
        "        unconditional_tokens = tf.convert_to_tensor(\n",
        "            [_UNCONDITIONAL_TOKENS], dtype=tf.int32\n",
        "        )\n",
        "        unconditional_context = self.encode_text_2(unconditional_tokens, self._get_pos_ids())\n",
        "\n",
        "        return unconditional_context\n",
        "    \n",
        "    def get_index_of_name(self, tensors, name):\n",
        "        for a in tensors:\n",
        "            if a['name'] == name:\n",
        "                return a['index']\n",
        "\n",
        "    def diffusion_model(self, latent, t_emb, unconditional_context):\n",
        "        i_diffusion = tf.lite.Interpreter('diffusion_model.tflite')\n",
        "        # batch_size = latent.shape[0]\n",
        "\n",
        "        i_diffusion_input_details = i_diffusion.get_input_details()\n",
        "        i_diffusion_output_details = i_diffusion.get_output_details()\n",
        "\n",
        "        i_diffusion.allocate_tensors()\n",
        "\n",
        "        for i in range(len(i_diffusion_input_details)):\n",
        "            if len(i_diffusion_input_details[i]['shape'].tolist()) == 4:\n",
        "                # i_diffusion.resize_tensor_input(i, latent.get_shape().as_list())\n",
        "                i_diffusion.set_tensor(i_diffusion_input_details[i]['index'], latent)\n",
        "            elif len(i_diffusion_input_details[i]['shape'].tolist()) == 3:\n",
        "                # i_diffusion.resize_tensor_input(i, unconditional_context.get_shape().as_list())\n",
        "                i_diffusion.set_tensor(i_diffusion_input_details[i]['index'], unconditional_context)\n",
        "            else:\n",
        "                # i_diffusion.resize_tensor_input(i, t_emb.get_shape().as_list())\n",
        "                i_diffusion.set_tensor(i_diffusion_input_details[i]['index'], t_emb)\n",
        "\n",
        "        i_diffusion.invoke()\n",
        "        \n",
        "        output_data = i_diffusion.get_tensor(i_diffusion_output_details[0]['index'])\n",
        "        return output_data\n",
        "    \n",
        "    def decode(self, encoded_image):\n",
        "        i_decoder = tf.lite.Interpreter('image_decoder.tflite')\n",
        "        i_decoder.allocate_tensors()\n",
        "        input_details = i_decoder.get_input_details()\n",
        "        output_details = i_decoder.get_output_details()\n",
        "\n",
        "        i_decoder.set_tensor(input_details[0]['index'], encoded_image)\n",
        "\n",
        "        i_decoder.invoke()\n",
        "\n",
        "        output_data = i_decoder.get_tensor(output_details[0]['index'])\n",
        "\n",
        "        return output_data\n",
        "\n",
        "    def generate_image(\n",
        "        self,\n",
        "        encoded_text,\n",
        "        negative_prompt=None,\n",
        "        batch_size=1,\n",
        "        num_steps=50,\n",
        "        unconditional_guidance_scale=7.5,\n",
        "        diffusion_noise=None,\n",
        "        seed=None,\n",
        "    ):\n",
        "        context = self._expand_tensor(encoded_text, batch_size)\n",
        "\n",
        "        if negative_prompt is None:\n",
        "            unconditional_context = tf.repeat(\n",
        "                self._get_unconditional_context(), batch_size, axis=0\n",
        "            )\n",
        "        else:\n",
        "            unconditional_context = self.encode_text(negative_prompt)\n",
        "            unconditional_context = self._expand_tensor(\n",
        "                unconditional_context, batch_size\n",
        "            )\n",
        "        if diffusion_noise is not None:\n",
        "            diffusion_noise = tf.squeeze(diffusion_noise)\n",
        "            if diffusion_noise.shape.rank == 3:\n",
        "                diffusion_noise = tf.repeat(\n",
        "                    tf.expand_dims(diffusion_noise, axis=0), batch_size, axis=0\n",
        "                )\n",
        "            latent = diffusion_noise\n",
        "        else:\n",
        "            latent = self._get_initial_diffusion_noise(batch_size, seed)\n",
        "\n",
        "        timesteps = tf.range(1, 1000, 1000 // num_steps)\n",
        "        alphas, alphas_prev = self._get_initial_alphas(timesteps)\n",
        "        progbar = keras.utils.Progbar(len(timesteps))\n",
        "        iteration = 0\n",
        "        for index, timestep in list(enumerate(timesteps))[::-1]:\n",
        "            latent_prev = latent  # Set aside the previous latent vector\n",
        "            t_emb = self._get_timestep_embedding(timestep, batch_size)\n",
        "            unconditional_latent = self.diffusion_model(latent, t_emb, unconditional_context)\n",
        "            \n",
        "            latent = self.diffusion_model(latent, t_emb, context)\n",
        "            latent = unconditional_latent + unconditional_guidance_scale * (\n",
        "                latent - unconditional_latent\n",
        "            )\n",
        "            a_t, a_prev = alphas[index], alphas_prev[index]\n",
        "            pred_x0 = (latent_prev - math.sqrt(1 - a_t) * latent) / math.sqrt(a_t)\n",
        "            latent = latent * math.sqrt(1.0 - a_prev) + math.sqrt(a_prev) * pred_x0\n",
        "            iteration += 1\n",
        "            progbar.update(iteration)\n",
        "\n",
        "        # Decoding stage\n",
        "        decoded = self.decode(latent)\n",
        "        decoded = ((decoded + 1) / 2) * 255\n",
        "        return np.clip(decoded, 0, 255).astype(\"uint8\")"
      ],
      "metadata": {
        "id": "ksnuMrMcavRg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_t = StableDiffusionTFLite()\n",
        "encoded_text_tflite = model_t.encode_text('a photo of an astronaut riding a horse on Mars')\n",
        "images = model_t.generate_image(encoded_text_tflite, num_steps=25) # Takes a while to generate (~30 mins)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2J7i2bIHb4Q1",
        "outputId": "317c2a00-c564-4823-a242-4e9dbba78e73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 5/25 [=====>........................] - ETA: 23:44"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image \n",
        "\n",
        "for i in range(len(images)):\n",
        "    image = Image.fromarray(images[i])\n",
        "    image.save(f\"{i}.png\")"
      ],
      "metadata": {
        "id": "pBhpHu8wc2B3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "scratchpad",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}